<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8" />
  
  <title>Seite 12 | YBC HomePage</title>
  <meta name="author" content="BaiChuan Yang" />

  
  <meta name="description" content="Blog mainly about IT technology and Interesting Life Events~" />
  

  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

  
  <meta property="og:site_name" content="YBC HomePage" />

  <meta name="google-site-verification" content="IK-tjIwORWUinhhD-XZkyX5FWQgd568VBgmJorjK7Bg" />
  <meta name="baidu-site-verification" content="iAx4x1uv2O" />
  <meta name="msvalidate.01" content="E6F8036020E5423AA7576A75B806937B" />  

  
    <meta property="og:image" content="undefined" />
  

  
  <link href="/css/images/favicon.ico" rel="icon" />
  

  <link rel="alternate" href="/atom.xml" title="YBC HomePage" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-63602183-1', 'auto');
	ga('send', 'pageview');

</script>



  <!-- baidu webmaster push -->
  <script src='//push.zhanzhang.baidu.com/push.js'></script>
</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">YBC HomePage</a></h1>
  <h2><a href="/">Study~ Work~ Life~ Everything is here~</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/about">About</a></li>
    
      <li><a href="/atom.xml">RSS</a></li>
    
      <li><a href="/sitemap.xml">SiteMap</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');
  
  _st('install','xeEkJ65dmswemsvZM1Nx','2.0.0');
</script>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-09-05T06:37:04.000Z"><a href="/2015/09/04/Storm-Install-and-Basic-Commands/">2015-09-04</a></time>
      
      
  
    <h1 class="title"><a href="/2015/09/04/Storm-Install-and-Basic-Commands/">Storm Install and Basic Commands</a></h1>
  

    </header>
    <div class="entry">
      
      
        
      
    </div>
    
    <footer>
        <div class="alignright">
          
          <a href='javascript:void(0)' class="share-link bdsharebuttonbox" data-cmd="more">Share</a>
        </div>
        
        
  
  <div class="tags">
    <a href="/tags/Cloud-Computing/">Cloud Computing</a>
  </div>

        <!-- partial('post/share') -->
      <div class="clearfix"></div>
    </footer>
  </div>
</article>






  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-09-04T03:30:53.000Z"><a href="/2015/09/03/Hadoop-1.x-Basic-Commands/">2015-09-03</a></time>
      
      
  
    <h1 class="title"><a href="/2015/09/03/Hadoop-1.x-Basic-Commands/">Hadoop Basic Commands</a></h1>
  

    </header>
    <div class="entry">
      
      
        <p>Here is the link of official hadoop command document. <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/CommandsManual.html#Overview" target="_blank" rel="external">Hadoop Command Official Document</a></p>
<h3 id="Hadoop-DistCp-Command"><a href="#Hadoop-DistCp-Command" class="headerlink" title="Hadoop DistCp Command"></a>Hadoop DistCp Command</h3><p>This kind of command is used for large inter/intra-cluster copying.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop distcp &lt;<span class="built_in">source</span> uri&gt; &lt;dest uri&gt;</span><br></pre></td></tr></table></figure></p>
<hr>

<h3 id="Hadoop-FileSystem-Command"><a href="#Hadoop-FileSystem-Command" class="headerlink" title="Hadoop FileSystem Command"></a>Hadoop FileSystem Command</h3><p>Append single src, or multiple srcs from local file system to the destination file system. Also reads input from stdin and appends to destination file system.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -appendToFile src[src ...] dst</span><br></pre></td></tr></table></figure></p>
<p>Print out the content of a specific file in HDFS<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -cat URI[URI ...]</span><br></pre></td></tr></table></figure></p>
<p>Returns the checksum information of a file.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -checksum URI</span><br></pre></td></tr></table></figure></p>
<p>Change the permissions of files.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI[URI ...]</span><br></pre></td></tr></table></figure></p>
<p>Change the owner of files.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</span><br></pre></td></tr></table></figure></p>
<p>Copy files from source to destination.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadopp fs -cp [<span class="_">-f</span>] [-p | -p[topax]] URI [URI ...] &lt;dest&gt;</span><br></pre></td></tr></table></figure></p>
<p>Finds all files that match the specified expression and applies selected actions to them.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -find &lt;path&gt; ... &lt;expression&gt; ...</span><br></pre></td></tr></table></figure></p>
<p>Copy local files to HDFS. Similar to put command, except that the source is restricted to a local file reference.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -copyFromLocal &lt;localsrc&gt; URI</span><br></pre></td></tr></table></figure></p>
<p>Copy files to the local file system. Files that fail the CRC check may be copied with the -ignorecrc option. Files and CRCs may be copied using the -crc option.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -get [-ignorecrc] [-crc] &lt;src&gt; &lt;localdst&gt;</span><br></pre></td></tr></table></figure></p>
<p>Copy files from HDS to local. Similar to get command, except that the destination is restricted to a local file reference.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -copyToLocal [-ignorecrc] [-crc] URI &lt;localdst&gt;</span><br></pre></td></tr></table></figure></p>
<p>Get usage output<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop <span class="built_in">help</span></span><br></pre></td></tr></table></figure></p>
<p>Returns the state of file or children under directory<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -ls [<span class="_">-d</span>] [-h] [-R] [-t] [-S] [-r] [-u] &lt;args&gt;</span><br></pre></td></tr></table></figure></p>
<p>Takes path uri’s as argument and creates directories.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -mkdir [-p] &lt;paths&gt;</span><br></pre></td></tr></table></figure></p>
<p>Move files from local to HDFS. Similar to put command, except that the source localsrc is deleted after it’s copied.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -moveFromLocal &lt;localsrc&gt; &lt;dst&gt;</span><br></pre></td></tr></table></figure></p>
<p>Move files from HDFS to local.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -moveToLocal [-crc] &lt;src&gt; &lt;dst&gt;</span><br></pre></td></tr></table></figure></p>
<p>Move files from source to destination.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -mv URI [URI ...] &lt;dest&gt;</span><br></pre></td></tr></table></figure></p>
<p>Copy single src, or multiple srcs from local file system to the destination file system. Also reads input from stdin and writes to destination file system.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -put &lt;localsrc&gt; ... &lt;dst&gt;</span><br></pre></td></tr></table></figure></p>
<p>Delete files specified as args.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -rm [<span class="_">-f</span>] [-r |-R] [-skipTrash] URI [URI ...]</span><br></pre></td></tr></table></figure></p>
<p>Delete a directory.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -rmdir [--ignore-fail-on-non-empty] URI [URI ...]</span><br></pre></td></tr></table></figure></p>
<p>Recursive version of delete.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -rmr [-skipTrash] URI [URI ...]</span><br></pre></td></tr></table></figure></p>
<p>Print statistics about the file/directory at <path></path> in the specified format. Format accepts filesize in blocks (%b), type (%F), group name of owner (%g), name (%n), block size (%o), replication (%r), user name of owner(%u), and modification date (%y, %Y). %y shows UTC date as “yyyy-MM-dd HH:mm:ss” and %Y shows milliseconds since January 1, 1970 UTC. If the format is not specified, %y is used by default.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -stat [format] &lt;path&gt; ...</span><br></pre></td></tr></table></figure></p>
<p>Displays last kilobyte of the file to stdout.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -tail [<span class="_">-f</span>] URI</span><br></pre></td></tr></table></figure></p>
<p>Takes a source file and outputs the file in text format.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -text &lt;src&gt;</span><br></pre></td></tr></table></figure></p>
<p>Truncate all files that match the specified file pattern to the specified length.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -truncate [-w] &lt;length&gt; &lt;paths&gt;</span><br></pre></td></tr></table></figure></p>
<p>Get the size of files/folders in human readible format:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop fs -du -h URI</span><br></pre></td></tr></table></figure></p>
<hr>

<h3 id="Runs-jar-files"><a href="#Runs-jar-files" class="headerlink" title="Runs jar files"></a>Runs jar files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop jar &lt;jar&gt; [mainClass] args...</span><br></pre></td></tr></table></figure>
<hr>

<h3 id="Hadoop-Version-Command"><a href="#Hadoop-Version-Command" class="headerlink" title="Hadoop Version Command"></a>Hadoop Version Command</h3><p>Prints the version of Hadoop system<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop version</span><br></pre></td></tr></table></figure></p>
<hr>

<h3 id="Hadoop-CLASSNAME"><a href="#Hadoop-CLASSNAME" class="headerlink" title="Hadoop CLASSNAME"></a>Hadoop CLASSNAME</h3><p>Runs the class named CLASSNAME<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop CLASSNAME</span><br></pre></td></tr></table></figure></p>

      
    </div>
    
    <footer>
        <div class="alignright">
          
          <a href='javascript:void(0)' class="share-link bdsharebuttonbox" data-cmd="more">Share</a>
        </div>
        
        
  
  <div class="tags">
    <a href="/tags/Cloud-Computing/">Cloud Computing</a>
  </div>

        <!-- partial('post/share') -->
      <div class="clearfix"></div>
    </footer>
  </div>
</article>






  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-09-04T03:29:42.000Z"><a href="/2015/09/03/Pig-Install-and-Basic-Commands/">2015-09-03</a></time>
      
      
  
    <h1 class="title"><a href="/2015/09/03/Pig-Install-and-Basic-Commands/">Pig Install and Basic Commands</a></h1>
  

    </header>
    <div class="entry">
      
      
        <p>Here is the official website of <a href="https://pig.apache.org/" target="_blank" rel="external">Apache Pig</a>. The following is the tutorial of pig installation and summary of basic commands.</p>
<h3 id="Pig-Intallation"><a href="#Pig-Intallation" class="headerlink" title="Pig Intallation"></a>Pig Intallation</h3><p>Download the pig from this <a href="https://pig.apache.org/releases.html#Download" target="_blank" rel="external">link</a>. Just choose any one release. In this tutorial, we use Pig-0.14.0.</p>
<p>Then unpacked the distribution using these commands:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -xzvf pig-0.14.0.tar.gz</span><br><span class="line">mv pig-0.14.0 <span class="string">"the directory you would like to install pig"</span></span><br></pre></td></tr></table></figure></p>
<p>Add pig-0.14.0/bin to environment path.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.bashrc</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="string">"path to pig project directory"</span>/bin</span><br></pre></td></tr></table></figure></p>
<p>Quit vim editor and “source” your .bashrc file.</p>
<p>Run this command to test whether pig works or not:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pig -help</span><br></pre></td></tr></table></figure></p>
<h3 id="Running-Pig"><a href="#Running-Pig" class="headerlink" title="Running Pig"></a>Running Pig</h3><p>Pig can be run in two modes, namely Local Mode and Mapreduce Mode.<br>In local mode, we can access to a single machine and all files are installed and run using your local host and file system.<br>In mapreduce mode, we can access to a Hadoop cluster and HDFS installation and this mode is the default mode. In this mode, it is required that Hadoop and HDFS should be running.</p>
<p>To run pig in local, using this command:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pig -x <span class="built_in">local</span></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">java -cp pig.jar org.apache.pig.Main -x <span class="built_in">local</span> <span class="comment">#java command</span></span><br></pre></td></tr></table></figure></p>
<p>To run pig in mapreduce mode, using this command:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pig</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">pig -x mapreduce</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">java -cp pig.jar org.apache.pig.Main  <span class="comment">#java command</span></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">java -cp pig.jar org.apache.pig.Main -x mapreduce <span class="comment">#java command</span></span><br></pre></td></tr></table></figure></p>
<p>Moreover, we can run pig in interactive mode using the Grunt shell through this command:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pig</span><br></pre></td></tr></table></figure></p>
<p>and then enter Pig Latin statements and Pig commands interactively at the command line.</p>

      
    </div>
    
    <footer>
        <div class="alignright">
          
          <a href='javascript:void(0)' class="share-link bdsharebuttonbox" data-cmd="more">Share</a>
        </div>
        
        
  
  <div class="tags">
    <a href="/tags/Cloud-Computing/">Cloud Computing</a>
  </div>

        <!-- partial('post/share') -->
      <div class="clearfix"></div>
    </footer>
  </div>
</article>






  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-09-04T02:54:03.000Z"><a href="/2015/09/03/Zookeeper-Install-and-Basic-Commands/">2015-09-03</a></time>
      
      
  
    <h1 class="title"><a href="/2015/09/03/Zookeeper-Install-and-Basic-Commands/">Zookeeper Install and Basic Commands</a></h1>
  

    </header>
    <div class="entry">
      
      
        <p>Here is a simple tutorial of Zookeeper installation and a summary of its basic commands. Note that if we do not use root account, we may have to use “sudo” command to execute some commands or grant read, write or execution access to the account we use.</p>
<h3 id="Zookeeper-Installation"><a href="#Zookeeper-Installation" class="headerlink" title="Zookeeper Installation"></a>Zookeeper Installation</h3><p>Here is the link of official <a href="https://zookeeper.apache.org/doc/r3.1.2/zookeeperStarted.html" target="_blank" rel="external">tutorial</a>. The following is my tutorial.</p>
<p>Environment: Ubuntu 14.04<br>Zookeeper: zookeeper 3.4.6</p>
<h5 id="Downlaod-and-Unpack-Zookeeper"><a href="#Downlaod-and-Unpack-Zookeeper" class="headerlink" title="Downlaod and Unpack Zookeeper"></a>Downlaod and Unpack Zookeeper</h5><p>Here is the link to download <a href="http://zookeeper.apache.org/releases.html" target="_blank" rel="external">stable release</a> of zookeeper.</p>
<p>Then unpack the tar.gz file:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf zookeeper-x.x.x.tar.gz</span><br></pre></td></tr></table></figure></p>
<p>After that, move the project directory to location we want.</p>
<h5 id="Edit-configuration-file"><a href="#Edit-configuration-file" class="headerlink" title="Edit configuration file"></a>Edit configuration file</h5><p>Go to zookeeper project directory and create a configure file using this command:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi conf/zoo.cfg</span><br></pre></td></tr></table></figure></p>
<p>Note that we may find zoo_sample.cfg file instead. In this case, we should change the name of that file to zoo.cfg.</p>
<p>And add following content into that file:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000 <span class="comment">#It is used to do heartbeats and the minimum session timeout will be twice the tickTime. The basic unit is milliseconds.</span></span><br><span class="line">dataDir=/var/zookeeper <span class="comment">#Here is the directory where zookeeper save its data.</span></span><br><span class="line">clientPort=2181 <span class="comment">#The port to listen for client connections.</span></span><br></pre></td></tr></table></figure></p>
<p>Note that we can create /va/zookeeper directory and change the read, write and execution access before we start the server.</p>
<p>Now we have successfully setted up a Zookeeper server in standalone mode.</p>
<h5 id="Start-Restart-Stop-and-Connect-to-the-server"><a href="#Start-Restart-Stop-and-Connect-to-the-server" class="headerlink" title="Start, Restart, Stop and Connect to the server"></a>Start, Restart, Stop and Connect to the server</h5><p>The command to start the server:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/zkServer.sh start</span><br></pre></td></tr></table></figure></p>
<p>To test whether the server is working or not, use this command:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure></p>
<p>And we should see a similar output like this:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">6834 JPS</span><br><span class="line">6997 QuorumPeerMain</span><br></pre></td></tr></table></figure></p>
<p>The command to restart the server:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/zkServer.sh restart</span><br></pre></td></tr></table></figure></p>
<p>The command to stop the server:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/zkServer.sh stop</span><br></pre></td></tr></table></figure></p>
<p>The command to connect to the server:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Java</span></span><br><span class="line">bin/zkCli.sh 127.0.0.1:2181</span><br><span class="line"></span><br><span class="line"><span class="comment">#C/C++</span></span><br><span class="line">LD_LIBRARY_PATH=. cli_mt 127.0.0.1:2181</span><br><span class="line"><span class="comment">#or</span></span><br><span class="line">LD_LIBRARY_PATH=. cli_st 127.0.0.1:2181</span><br></pre></td></tr></table></figure></p>

      
    </div>
    
    <footer>
        <div class="alignright">
          
          <a href='javascript:void(0)' class="share-link bdsharebuttonbox" data-cmd="more">Share</a>
        </div>
        
        
  
  <div class="tags">
    <a href="/tags/Cloud-Computing/">Cloud Computing</a>
  </div>

        <!-- partial('post/share') -->
      <div class="clearfix"></div>
    </footer>
  </div>
</article>






  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-09-04T02:53:52.000Z"><a href="/2015/09/03/Hbase-Installation-Guide/">2015-09-03</a></time>
      
      
  
    <h1 class="title"><a href="/2015/09/03/Hbase-Installation-Guide/">HBase Installation Guide</a></h1>
  

    </header>
    <div class="entry">
      
      
        <p>Here is the official guide to <a href="https://hbase.apache.org/book.html#quickstart" target="_blank" rel="external">install HBase</a>. Below is the installation guide for HBase 0.94.</p>
<hr>

<h3 id="Environment-Setup"><a href="#Environment-Setup" class="headerlink" title="Environment Setup"></a>Environment Setup</h3><p>Prior to HBase 0.94.x, HBase expected the loopback IP address to be 127.0.0.1 but Ubuntu and some other distributions default to 127.0.1.1, which will cause problems. So we have to check /etc/hosts first and its content should be as follows:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1 localhost</span><br><span class="line">127.0.0.1 ubuntu.ubuntu-domain ubuntu</span><br></pre></td></tr></table></figure></p>
<p>JDK is required by HBase. Install required JDK version on the OS. Please do remember to set JAVA_HOME variable in the system.</p>
<hr>

<h3 id="Install-HBase-in-Standalone-Mode"><a href="#Install-HBase-in-Standalone-Mode" class="headerlink" title="Install HBase in Standalone Mode"></a>Install HBase in Standalone Mode</h3><p>Download HBase binary packages from <a href="http://www.apache.org/dyn/closer.cgi/hbase/" target="_blank" rel="external">Apache Download Mirror</a> or we can download source code and build it.</p>
<p>Here we choose to download binary package and extract it:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar xzvf hbase-&lt;?<span class="built_in">eval</span> <span class="variable">$&#123;project.version&#125;</span>?&gt;-bin.tar.gz</span><br><span class="line"><span class="built_in">cd</span> hbase-&lt;?<span class="built_in">eval</span> <span class="variable">$&#123;project.version&#125;</span>?&gt;/</span><br></pre></td></tr></table></figure></p>
<p>Edit hbase-site.xml file under conf folder, and put following content into that xml file beneath the <configuration> tags:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;file:///home/yular/hbase&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/yular/zookeeper&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></configuration></p>
<p>Note that file:///home/yular/hbase is the local filesystem directory where HBase and ZooKeeper write data. By default, a new directory is created under /tmp but all the data will be removed when os is reboot.</p>
<p>To start HBase, use this command:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/start-hbase.sh</span><br></pre></td></tr></table></figure></p>
<p>If HBase starts successfully, execute jps command and output should be similar like this:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">3145 Jps</span><br><span class="line">6678 HMaster</span><br></pre></td></tr></table></figure></p>
<p>Note that a HMaster process is running.</p>
<p>And then we can start HBase shell like this:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hbase shell</span><br></pre></td></tr></table></figure></p>
<p>The HBase terminal should be like this:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt;</span><br></pre></td></tr></table></figure></p>
<hr>

<h3 id="Install-HBase-in-Pseudo-Distributed-Local-Mode"><a href="#Install-HBase-in-Pseudo-Distributed-Local-Mode" class="headerlink" title="Install HBase in Pseudo-Distributed Local Mode"></a>Install HBase in Pseudo-Distributed Local Mode</h3><p>At first, stop HBase:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/stop-hbase.sh</span><br></pre></td></tr></table></figure></p>
<p>Install and start Hadoop2 system. Here is the tutorial to <a href="http://yular.github.io/2015/09/02/Hadoop-2-x-Single-Node-Cluster-Step-Up-Guide/">install and start Hadoop2</a>.</p>
<p>Then hbase-site.xml file under conf folder, and put following content into that xml file beneath the <configuration> tags:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/yular/zookeeper&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></configuration></p>
<p>Note that the port number in hbase.rootdir should be the same as the one defined in hadoop2 etc/hadoop/ core-site.xml file:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.default.name&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<p>Note that we do not need to create hbase folder in HDFS system. When we start HBase, hbase folder will be generated initially.</p>
<p>To start HBase, use this command (Hadoop2 should be running):<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/start-hbase.sh</span><br></pre></td></tr></table></figure></p>
<p>If HBase is running successfully, the output of jps command should be like this:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2055 ResourceManager</span><br><span class="line">1149 Jps</span><br><span class="line">3248 NodeManager</span><br><span class="line">5093 NameNode</span><br><span class="line">3799 DataNode</span><br><span class="line">1190 HRegionServer</span><br><span class="line">2467 JobHistoryServer</span><br><span class="line">3120 HMaster</span><br></pre></td></tr></table></figure></p>
<p>Note there are two processes relative to HBase running: HRegionServer and HMaster.</p>
<p>If we can visit <a href="http://localhost:60010" target="_blank" rel="external">http://localhost:60010</a>, we can visit HBase job summary page.</p>
<p>And if we execute hadoop fs -ls /hbase command, we will some folders that have been created by HBase:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Found 7 items</span><br><span class="line">drwxr-xr-x   - hbase users          0 2014-06-25 18:58 /hbase/.tmp</span><br><span class="line">drwxr-xr-x   - hbase users          0 2014-06-25 21:49 /hbase/WALs</span><br><span class="line">drwxr-xr-x   - hbase users          0 2014-06-25 18:48 /hbase/corrupt</span><br><span class="line">drwxr-xr-x   - hbase users          0 2014-06-25 18:58 /hbase/data</span><br><span class="line">-rw-r--r--   3 hbase users         42 2014-06-25 18:41 /hbase/hbase.id</span><br><span class="line">-rw-r--r--   3 hbase users          7 2014-06-25 18:41 /hbase/hbase.version</span><br><span class="line">drwxr-xr-x   - hbase users          0 2014-06-25 21:49 /hbase/oldWALs</span><br></pre></td></tr></table></figure></p>

      
    </div>
    
    <footer>
        <div class="alignright">
          
          <a href='javascript:void(0)' class="share-link bdsharebuttonbox" data-cmd="more">Share</a>
        </div>
        
        
  
  <div class="tags">
    <a href="/tags/Cloud-Computing/">Cloud Computing</a>
  </div>

        <!-- partial('post/share') -->
      <div class="clearfix"></div>
    </footer>
  </div>
</article>






  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-09-04T02:53:43.000Z"><a href="/2015/09/03/Hive-Install-and-Basic-Commands/">2015-09-03</a></time>
      
      
  
    <h1 class="title"><a href="/2015/09/03/Hive-Install-and-Basic-Commands/">Hive Install and Basic Commands</a></h1>
  

    </header>
    <div class="entry">
      
      
        <p>Here is the official website of <a href="https://cwiki.apache.org/confluence/display/Hive/Home" target="_blank" rel="external">Hive</a>.</p>
<p>###Hive Install<br>Here is the official document of <a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="external">Hive Installation</a>.</p>
<p>Now the following is the guide of Hive installation.</p>
<p>#####Uncompress hive package<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tar -xzvf hive-x.y.z.tar.gz</span><br></pre></td></tr></table></figure></p>
<p>#####Set Hive environment variable<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ vi ~/.bashrc</span><br><span class="line">$ <span class="built_in">export</span> HIVE_HOME=<span class="string">'The absolute path of hive home directory'</span></span><br><span class="line">$ <span class="built_in">export</span> PATH=<span class="variable">$HIVE_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure></p>
<hr>



      
    </div>
    
    <footer>
        <div class="alignright">
          
          <a href='javascript:void(0)' class="share-link bdsharebuttonbox" data-cmd="more">Share</a>
        </div>
        
        
  
  <div class="tags">
    <a href="/tags/Cloud-Computing/">Cloud Computing</a>
  </div>

        <!-- partial('post/share') -->
      <div class="clearfix"></div>
    </footer>
  </div>
</article>






  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-09-03T03:28:44.000Z"><a href="/2015/09/02/Hadoop-2-x-Single-Node-Cluster-Step-Up-Guide/">2015-09-02</a></time>
      
      
  
    <h1 class="title"><a href="/2015/09/02/Hadoop-2-x-Single-Node-Cluster-Step-Up-Guide/">Hadoop 2.x Single-Node Cluster Set Up Guide</a></h1>
  

    </header>
    <div class="entry">
      
      
        <p>Now the following is a lazy set up guide for those who only would like to set up a single-node cluster for some easy tasks, like school homework. This guide should be much quicker than the one recommended above.</p>
<p>Here is a good and more detailed <a href="http://bigdatahandler.com/hadoop-hdfs/installing-single-node-hadoop-2-2-0-on-ubuntu/" target="_blank" rel="external">tutorial</a> from Horton. But they missed an important step which may bring some trouble for beginners.</p>
<p>Environment: Ubuntu 14.04 LTS<br>Hadoop: 2.7.1</p>
<hr>

<p>#####Java Installation<br>Hadoop requires Java 1.5+ installation. So just go to Oracle Java home page to download JDK 1.6 or higher. Here is the link to download Java: <a href="http://www.oracle.com/technetwork/articles/javase/index-jsp-138363.html" target="_blank" rel="external">Java Download</a>. Or we can just the following commands:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install openjdk-7-jdk</span><br></pre></td></tr></table></figure></p>
<p>Here is a tutorial <a href="https://www.digitalocean.com/community/tutorials/how-to-install-java-on-ubuntu-with-apt-get" target="_blank" rel="external">Java Install</a>.</p>
<hr>

<p>#####(Optional)Adding a Hadoop system user<br>Use the following commands to add a new user and a new group for Hadoop system.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo addgroup hadoop</span><br><span class="line">$ sudo adduser --ingroup hadoop newuser</span><br></pre></td></tr></table></figure></p>
<p>Actually, we can use the existing user and group. If so, just skip this step.</p>
<hr>

<p>#####SSH key<br>Here are the commands to install ssh if our machine does not have one.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install ssh</span><br><span class="line">$ sudo apt-get install rsync</span><br></pre></td></tr></table></figure></p>
<p>After that, generate SSH key for the user:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ su - newuser</span><br><span class="line">$ ssh-keygen -t rsa -P <span class="string">""</span></span><br><span class="line">$ cat <span class="variable">$HOME</span>/.ssh/id_rsa.pub &gt;&gt; <span class="variable">$GINE</span>/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p>
<p>Then, do the ssh connection:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh localhost</span><br></pre></td></tr></table></figure></p>
<hr>

<p>#####(Optional)Disabling IPv6<br>Just go to the Hadoop setup tutorial provided in the begining.</p>
<hr>

<p>#####Hadoop Download and Installation<br>Go to <a href="http://hadoop.apache.org/releases.html" target="_blank" rel="external">Apache Hadoop Homepage</a> to download the Hadoop from one of the mirrors. Then we can put the Hadoop package to a location of our machine. The location can be /usr/local or even our home directory. Here I choose /usr/local.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /usr/<span class="built_in">local</span></span><br><span class="line">$ sudo tar xzvf hadoop-2.7.1.tar.gz</span><br><span class="line">$ sudo mv hadoop-2.7.1 hadoop</span><br><span class="line">$ sudo chown -R newuser:hadoop hadoop</span><br></pre></td></tr></table></figure>
<p>Now a single-node hadoop is available and if we type command:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop/bin/hadoop</span><br></pre></td></tr></table></figure>
<p>A list of information about “hadoop” command will appear.</p>
<hr>

<p>#####Update Environment Variable<br>Editing the .bashrc file under the home directory of Hadoop system user. Adding these two lines to that file:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_PREFIX=/usr/<span class="built_in">local</span>/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span></span><br><span class="line"><span class="built_in">export</span> YARN_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_LIB_NATIVE_DIR=<span class="variable">$&#123;HADOOP_PREFIX&#125;</span>/lib/native</span><br><span class="line"><span class="built_in">export</span> HADOOP_OPTS=<span class="string">"-Djava.library.path=<span class="variable">$HADOOP_PREFIX</span>/lib"</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="string">"the path to your java jdk directory"</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure></p>
<p>In fact, in Hadoop-1.2.1, HADOOP_HOME variable is deprecated and we can skip this step to set it. If have difficulty to find where the jdk is, this link may be helpful: <a href="http://stackoverflow.com/questions/5251323/where-can-i-find-the-java-sdk-in-linux" target="_blank" rel="external">JDK path</a>.</p>
<hr>

<p>#####Hadoop Distributed File System (HDFS)<br>At first, edit etc/hadoop/hadoop-env.sh file, by setting the JAVA_HOME and HADOOP_CONF_DIR variables:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> JAVA_HOME=.... (your jdk path)</span><br><span class="line">$ <span class="built_in">export</span> HADOOP_CONF_DIR=/usr/<span class="built_in">local</span>/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure></p>
<p>Note: If we forget to change the HADOOP_CONF_DIR variable, when we start HADOOP YARN system, an “Error: Can not find configuration directory: etc/hadoop” exception will appear. </p>
<p>Then create the two directories for Namenode and Datanode respectively and set the required ownerships as well as permissions:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /</span><br><span class="line">$ sudo mkdir -p <span class="variable">$HADOOP_HOME</span>/yarn_data/hdfs/namenode</span><br><span class="line">$ sudo chown newuser:hadoop <span class="variable">$HADOOP_HOME</span>/yarn_data/hdfs/namenode</span><br><span class="line">$ sudo mkdir -p <span class="variable">$HADOOP_HOME</span>/yarn_data/hdfs/datanode</span><br><span class="line">$ sudo chown newuser:hadoop <span class="variable">$HADOOP_HOME</span>/yarn_data/hdfs/datanode</span><br><span class="line"><span class="comment"># ...and if you want to tighten up security, chmod from 755 to 750...</span></span><br><span class="line">$ sudo chmod 750 <span class="variable">$HADOOP_HOME</span>/yarn_data/hdfs/namenode</span><br><span class="line">$ sudo chmod 750 <span class="variable">$HADOOP_HOME</span>/yarn_data/hdfs/datanode</span><br></pre></td></tr></table></figure></p>
<p>Note: If we use existing users in the machine, then remember to replace the username and group name above. In fact, group name is not necessary.<br>      If we forget to create this directory or grant the correct ownerships or permissions, a java.io.IOException will be thrown out.</p>
<p>At last, edit the following files in Hadoop project directory:<br>For etc/hadoop/yarn-site.xml, adding following context between “<configuration>  </configuration>“ tag:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<p>For etc/hadoop/core-site.xml, adding following context between “<configuration> </configuration>“ tag:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.default.name&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<p>For etc/hadoop/mapred-site.xml, adding following context between “<configuration>  </configuration>“ tag:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<p>For etc/hadoop/hdfs-site.xml, adding following context between “<configuration>  </configuration>“ tag:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;file:/usr/<span class="built_in">local</span>/hadoop/yarn_data/hdfs/namenode&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;file:/usr/<span class="built_in">local</span>/hadoop/yarn_data/hdfs/datanode&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<hr>

<p>#####Formating HDFS via NameNode<br>Use the following commands to format the HDFS:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/<span class="built_in">local</span>/hadoop/bin/hadoop namenode -format</span><br></pre></td></tr></table></figure></p>
<p>The output is very long and the end of it looks like this:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">09/01/15 16:59:56 INFO namenode.FSNamesystem: fsOwner=hduser,hadoop</span><br><span class="line">09/01/15 16:59:56 INFO namenode.FSNamesystem: supergroup=supergroup</span><br><span class="line">09/01/15 16:59:56 INFO namenode.FSNamesystem: isPermissionEnabled=<span class="literal">true</span></span><br><span class="line">09/01/15 16:59:56 INFO common.Storage: Image file of size 96 saved <span class="keyword">in</span> 0 seconds.</span><br><span class="line">09/01/15 16:59:57 INFO common.Storage: Storage directory .../hadoop-hduser/dfs/name has been successfully formatted.</span><br><span class="line">09/01/15 16:59:57 INFO namenode.NameNode: SHUTDOWN_MSG:</span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1</span><br><span class="line">************************************************************/</span><br></pre></td></tr></table></figure></p>
<p>Now, everything is ready~ We can start the Hadoop system now~</p>
<hr>

<p>#####Starting Single-Node Cluster<br>Use the following commands:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/<span class="built_in">local</span>/hadoop/sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></p>
<p>Now nodemanager and resourcemanager starts.</p>
<p>Then run these commands to start the rest of nodes:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/<span class="built_in">local</span>/hadoop/sbin/hadoop-daemon.sh start namenode</span><br><span class="line">$ /usr/<span class="built_in">local</span>/hadoop/sbin/hadoop-daemon.sh start datanode</span><br><span class="line">$ /usr/<span class="built_in">local</span>/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure></p>
<p>Now use “jps” command, the output should look like this:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ jps</span><br><span class="line">2085 ResourceManager</span><br><span class="line">2349 Jps</span><br><span class="line">1788 NodeManager</span><br><span class="line">1553 NameNode</span><br><span class="line">1788 DataNode</span><br><span class="line">1437 JobHistoryServer</span><br></pre></td></tr></table></figure></p>
<p>Go to this link:<br><a href="http://localhost:8088/" target="_blank" rel="external">http://localhost:8088/</a> ResourceManager</p>
<p>#####Stopping Single-Node Cluster<br>Use the following commands:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/<span class="built_in">local</span>/hadoop/sbin/stop-yarn.sh</span><br><span class="line">$ /usr/<span class="built_in">local</span>/hadoop/sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure></p>
<p>Here is two scripts that can make things easier:<br>The script to start Hadoop-2 cluster:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span><br><span class="line"></span></span><br><span class="line">sh /usr/<span class="built_in">local</span>/hadoop/sbin/start-yarn.sh</span><br><span class="line">sh /usr/<span class="built_in">local</span>/hadoop/sbin/hadoop-daemon.sh start namenode</span><br><span class="line">sh /usr/<span class="built_in">local</span>/hadoop/sbin/hadoop-daemon.sh start datanode</span><br><span class="line">sh /usr/<span class="built_in">local</span>/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure></p>
<p>The script to stop Hadoop-2 cluster:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sh /usr/<span class="built_in">local</span>/hadoop/sbin/stop-yarn.sh</span><br><span class="line">sh /usr/<span class="built_in">local</span>/hadoop/sbin/hadoop-daemon.sh stop namenode</span><br><span class="line">sh /usr/<span class="built_in">local</span>/hadoop/sbin/hadoop-daemon.sh stop datanode</span><br><span class="line">sh /usr/<span class="built_in">local</span>/hadoop/sbin/mr-jobhistory-daemon.sh stop historyserver</span><br></pre></td></tr></table></figure></p>
<p>This lazy guide is over. If you have any questions, just leave your comments on this blog. For more details, just go to the tutorial provided on the top of this blog.</p>

      
    </div>
    
    <footer>
        <div class="alignright">
          
          <a href='javascript:void(0)' class="share-link bdsharebuttonbox" data-cmd="more">Share</a>
        </div>
        
        
  
  <div class="tags">
    <a href="/tags/Cloud-Computing/">Cloud Computing</a>
  </div>

        <!-- partial('post/share') -->
      <div class="clearfix"></div>
    </footer>
  </div>
</article>






  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-09-02T03:37:47.000Z"><a href="/2015/09/01/Hadoop-1.x-Single-Node-Cluster-Step-Up-Guide/">2015-09-01</a></time>
      
      
  
    <h1 class="title"><a href="/2015/09/01/Hadoop-1.x-Single-Node-Cluster-Step-Up-Guide/">Hadoop 1.x Single-Node Cluster Set Up</a></h1>
  

    </header>
    <div class="entry">
      
      
        <p>Here is the link of a perfect tutorial for Hadoop Single-Node Cluster set up: <a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/" target="_blank" rel="external">Hadoop Setup Guide</a>.</p>
<p>Now the following is a lazy set up guide for those who only would like to set up a single-node cluster for some easy tasks, like school homework. This guide should be much quicker than the one recommended above.</p>
<p>Environment: Ubuntu 14.04 LTS<br>Hadoop: 1.2.1</p>
<hr>

<p>#####Java Installation<br>Hadoop requires Java 1.5+ installation. So just go to Oracle Java home page to download JDK 1.6 or higher. Here is the link to download Java: <a href="http://www.oracle.com/technetwork/articles/javase/index-jsp-138363.html" target="_blank" rel="external">Java Download</a>. Or we can just the following commands:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install openjdk-7-jdk</span><br></pre></td></tr></table></figure></p>
<p>Here is a tutorial <a href="https://www.digitalocean.com/community/tutorials/how-to-install-java-on-ubuntu-with-apt-get" target="_blank" rel="external">Java Install</a>.</p>
<hr>

<p>#####(Optional)Adding a Hadoop system user<br>Use the following commands to add a new user and a new group for Hadoop system.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo addgroup hadoop</span><br><span class="line">$ sudo adduser --ingroup hadoop newuser</span><br></pre></td></tr></table></figure></p>
<p>Actually, we can use the existing user and group. If so, just skip this step.</p>
<hr>

<p>#####SSH key<br>Here are the commands to install ssh if our machine does not have one.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install ssh</span><br><span class="line">$ sudo apt-get install rsync</span><br></pre></td></tr></table></figure></p>
<p>After that, generate SSH key for the user:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ su - newuser</span><br><span class="line">$ ssh-keygen -t rsa -P <span class="string">""</span></span><br><span class="line">$ cat <span class="variable">$HOME</span>/.ssh/id_rsa.puh &gt;&gt; <span class="variable">$NEWUSER_HOME</span>/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p>
<p>Then, do the ssh connection:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh localhost</span><br></pre></td></tr></table></figure></p>
<hr>

<p>#####(Optional)Disabling IPv6<br>Just go to the Hadoop setup tutorial provided in the begining.</p>
<hr>

<p>#####Hadoop Download and Installation<br>Go to <a href="http://hadoop.apache.org/releases.html" target="_blank" rel="external">Apache Hadoop Homepage</a> to download the Hadoop from one of the mirrors. Then we can put the Hadoop package to a location of our machine. The location can be /usr/local or even our home directory. Here I choose /usr/local.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /usr/<span class="built_in">local</span></span><br><span class="line">$ sudo tar xzvf hadoop-1.2.1.tar.gz</span><br><span class="line">$ sudo mv hadoop-1.2.1 hadoop</span><br><span class="line">$ sudo chown -R newuser:hadoop hadoop</span><br></pre></td></tr></table></figure>
<p>Now a single-node hadoop is available and if we type command:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop/bin/hadoop</span><br></pre></td></tr></table></figure></p>
<p>A list of information about “hadoop” command will appear.</p>
<hr>

<p>#####Update Environment Variable<br>Editing the .bashrc file under the home directory of Hadoop system user. Adding these two lines to that file:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="string">"the path to your java jdk directory"</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br></pre></td></tr></table></figure></p>
<p>In fact, in Hadoop-1.2.1, HADOOP_HOME variable is deprecated and we can skip this step to set it. If have difficulty to find where the jdk is, this link may be helpful: <a href="http://stackoverflow.com/questions/5251323/where-can-i-find-the-java-sdk-in-linux" target="_blank" rel="external">JDK path</a>.</p>
<hr>

<p>#####Hadoop Distributed File System (HDFS)<br>At first, edit conf/hadoop-env.sh file, which is under conf/ directory, by setting the JAVA_HOME variable:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> JAVA_HOME=... (your jdk path)</span><br></pre></td></tr></table></figure></p>
<p>Then create the /app/hadoop/tmp directory and set the required ownerships as well as permissions:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /</span><br><span class="line">$ sudo mkdir -p /app/hadoop/tmp</span><br><span class="line">$ sudo chown newuser:hadoop /app/hadoop/tmp</span><br><span class="line"><span class="comment"># ...and if you want to tighten up security, chmod from 755 to 750...</span></span><br><span class="line">$ sudo chmod 750 /app/hadoop/tmp</span><br></pre></td></tr></table></figure></p>
<p>Note: If we forget to create this directory or grant the correct ownerships or permissions, a java.io.IOException will be thrown out.<br>      If we use existing users in the machine, then remember to replace the username and group name above. In fact, group name is not necessary.</p>
<p>At last, edit the following files:<br>For conf/core-site.xml, adding following context between <configuration> … </configuration> tag:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/app/hadoop/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.default.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://localhost:54310&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<p>For conf/mapred-site.xml, adding following context between <configuration> … </configuration> tag:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapred.job.tracker&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;localhost:54311&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<p>For conf/hdfs-site.xml, adding following context between <configuration> … </configuration> tag:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Default block replication.</span><br><span class="line">  The actual number of replications can be specified when the file is created.</span><br><span class="line">  The default is used <span class="keyword">if</span> replication is not specified <span class="keyword">in</span> create time.</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<hr>

<p>#####Formating HDFS via NameNode<br>Use the following commands to format the HDFS:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/<span class="built_in">local</span>/hadoop/bin/hadoop namenode -format</span><br></pre></td></tr></table></figure></p>
<p>The output should look like this:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">09/01/15 16:59:56 INFO namenode.NameNode: STARTUP_MSG:</span><br><span class="line">/************************************************************</span><br><span class="line">STARTUP_MSG: Starting NameNode</span><br><span class="line">STARTUP_MSG:   host = ubuntu/127.0.1.1</span><br><span class="line">STARTUP_MSG:   args = [-format]</span><br><span class="line">STARTUP_MSG:   version = 0.20.2</span><br><span class="line">STARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-0.20 -r 911707; compiled by <span class="string">'chrisdo'</span> on Fri Feb 19 08:07:34 UTC 2010</span><br><span class="line">************************************************************/</span><br><span class="line">09/01/15 16:59:56 INFO namenode.FSNamesystem: fsOwner=hduser,hadoop</span><br><span class="line">09/01/15 16:59:56 INFO namenode.FSNamesystem: supergroup=supergroup</span><br><span class="line">09/01/15 16:59:56 INFO namenode.FSNamesystem: isPermissionEnabled=<span class="literal">true</span></span><br><span class="line">09/01/15 16:59:56 INFO common.Storage: Image file of size 96 saved <span class="keyword">in</span> 0 seconds.</span><br><span class="line">09/01/15 16:59:57 INFO common.Storage: Storage directory .../hadoop-hduser/dfs/name has been successfully formatted.</span><br><span class="line">09/01/15 16:59:57 INFO namenode.NameNode: SHUTDOWN_MSG:</span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1</span><br><span class="line">************************************************************/</span><br></pre></td></tr></table></figure></p>
<p>Now, everything is ready~ We can start the Hadoop system now~</p>
<hr>

<p>#####Starting Single-Node Cluster<br>Use the following commands:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/<span class="built_in">local</span>/hadoop/bin/start-all.sh</span><br></pre></td></tr></table></figure></p>
<p>The output should look like this:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">starting namenode, logging to /usr/<span class="built_in">local</span>/hadoop/bin/../logs/hadoop-newuser-namenode-ubuntu.out</span><br><span class="line">localhost: starting datanode, logging to /usr/<span class="built_in">local</span>/hadoop/bin/../logs/hadoop-newuser-datanode-ubuntu.out</span><br><span class="line">localhost: starting secondarynamenode, logging to /usr/<span class="built_in">local</span>/hadoop/bin/../logs/hadoop-newuser-secondarynamenode-ubuntu.out</span><br><span class="line">starting jobtracker, logging to /usr/<span class="built_in">local</span>/hadoop/bin/../logs/hadoop-newuser-jobtracker-ubuntu.out</span><br><span class="line">localhost: starting tasktracker, logging to /usr/<span class="built_in">local</span>/hadoop/bin/../logs/hadoop-newuser-tasktracker-ubuntu.out</span><br></pre></td></tr></table></figure></p>
<p>Now use “jps” command, the output should look like this:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ jps</span><br><span class="line">2287 TaskTracker</span><br><span class="line">2149 JobTracker</span><br><span class="line">1938 DataNode</span><br><span class="line">2085 SecondaryNameNode</span><br><span class="line">2349 Jps</span><br><span class="line">1788 NameNode</span><br></pre></td></tr></table></figure></p>
<p>Go to following links:<br><a href="http://localhost:50070/" target="_blank" rel="external">http://localhost:50070/</a> – web UI of the NameNode daemon<br><a href="http://localhost:50030/" target="_blank" rel="external">http://localhost:50030/</a> – web UI of the JobTracker daemon<br><a href="http://localhost:50060/" target="_blank" rel="external">http://localhost:50060/</a> – web UI of the TaskTracker daemon</p>
<p>#####Stopping Single-Node Cluster<br>Use the following commands:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/<span class="built_in">local</span>/hadoop/bin/stop-all.sh</span><br></pre></td></tr></table></figure></p>
<p>This lazy guide is over. If you have any questions, just leave your comments on this blog. For more details, just go to the tutorial provided on the top of this blog.</p>

      
    </div>
    
    <footer>
        <div class="alignright">
          
          <a href='javascript:void(0)' class="share-link bdsharebuttonbox" data-cmd="more">Share</a>
        </div>
        
        
  
  <div class="tags">
    <a href="/tags/Cloud-Computing/">Cloud Computing</a>
  </div>

        <!-- partial('post/share') -->
      <div class="clearfix"></div>
    </footer>
  </div>
</article>






  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-08-24T04:53:25.000Z"><a href="/2015/08/23/LeetCode-Problem-Solution-List-VI/">2015-08-23</a></time>
      
      
  
    <h1 class="title"><a href="/2015/08/23/LeetCode-Problem-Solution-List-VI/">LeetCode Problem Solution List VI</a></h1>
  

    </header>
    <div class="entry">
      
      
        <h3 id="First-Bad-Version"><a href="#First-Bad-Version" class="headerlink" title="First Bad Version"></a>First Bad Version</h3><p>Using binary search algorithm.<br>用二分搜索即可。<br><a href="https://github.com/yular/CC--InterviewProblem/blob/master/LeetCode/leetcode_first-bad-version.cpp" target="_blank" rel="external">Read Code</a></p>
<h3 id="Wiggle-Sort"><a href="#Wiggle-Sort" class="headerlink" title="Wiggle Sort"></a>Wiggle Sort</h3><p>A classic Google Interview problem. Start from index one, and then check whether is the largest one among its two adjacent numbers; if not, swap it with the largest one; after that go to the next two element until reaching the end.<br>经典的谷歌面试题。从第二个元素开始，比较当前元素和与其相邻两个元素的大小，将最大那个与它交换位置；然后每次移动到下两个元素，直到到达数组末尾。<br><a href="https://github.com/yular/CC--InterviewProblem/blob/master/LeetCode/leetcode_wigglesort.cpp" target="_blank" rel="external">Read Code</a></p>
<h3 id="Range-Sum-Query-Mutable"><a href="#Range-Sum-Query-Mutable" class="headerlink" title="Range Sum Query - Mutable"></a>Range Sum Query - Mutable</h3><p>A kind of data structure problem. We can use segment tree or binary index tree to solve it. For those who do not know what is segment tree, click <a href="https://en.wikipedia.org/wiki/Segment_tree" target="_blank" rel="external">here</a>; those who do not know binary index tree, click <a href="https://www.topcoder.com/community/data-science/data-science-tutorials/binary-indexed-trees/" target="_blank" rel="external">here</a>.<br>裸的数据结构题，用线段树或者树状数组即可。无需做任何变换，直接套模板即可。<br><a href="https://github.com/yular/CC--InterviewProblem/blob/master/LeetCode/leetcode_range-sum-query-mutable.cpp" target="_blank" rel="external">Read Code</a>.</p>
<h3 id="Range-Sum-Query-2D-Mutable"><a href="#Range-Sum-Query-2D-Mutable" class="headerlink" title="Range Sum Query 2D - Mutable"></a>Range Sum Query 2D - Mutable</h3><p>A data structure problem. Here we can use segement tree or binary index tree but both of them should be 2D. For those who do not know what is segment tree, click <a href="https://en.wikipedia.org/wiki/Segment_tree" target="_blank" rel="external">here</a>; those who do not know binary index tree, click <a href="https://www.topcoder.com/community/data-science/data-science-tutorials/binary-indexed-trees/" target="_blank" rel="external">here</a>.<br>裸的二维线段树或者树状数组题目，直接套模板即可，不需要做任何变换。<br><a href="https://github.com/yular/CC--InterviewProblem/blob/master/LeetCode/leetcode_range-sum-query-2d-mutable.cpp" target="_blank" rel="external">Read Code</a>.</p>

      
    </div>
    
    <footer>
        <div class="alignright">
          
          <a href='javascript:void(0)' class="share-link bdsharebuttonbox" data-cmd="more">Share</a>
        </div>
        
        
  
  <div class="tags">
    <a href="/tags/Algorithm/">Algorithm</a>
  </div>

        <!-- partial('post/share') -->
      <div class="clearfix"></div>
    </footer>
  </div>
</article>






  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-08-22T05:10:47.000Z"><a href="/2015/08/21/Linux-Commands-for-Network-Management/">2015-08-21</a></time>
      
      
  
    <h1 class="title"><a href="/2015/08/21/Linux-Commands-for-Network-Management/">Linux Commands for Network Management</a></h1>
  

    </header>
    <div class="entry">
      
      
        <p>#####Check and Do Network Configuration<br>Use ifconfig command<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ifconfig</span><br></pre></td></tr></table></figure></p>
<p>Note: A useful instruction of the output of this command <a href="http://www.cnblogs.com/peida/archive/2013/02/27/2934525.html" target="_blank" rel="external">click here</a>.</p>

      
    </div>
    
    <footer>
        <div class="alignright">
          
          <a href='javascript:void(0)' class="share-link bdsharebuttonbox" data-cmd="more">Share</a>
        </div>
        
        
  
  <div class="tags">
    <a href="/tags/Linux/">Linux</a>
  </div>

        <!-- partial('post/share') -->
      <div class="clearfix"></div>
    </footer>
  </div>
</article>







<nav id="pagination">
  
  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/page/11/">&laquo; Vorherige Seite</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="page-number" href="/page/11/">11</a><span class="page-number current">12</span><a class="page-number" href="/page/13/">13</a><a class="page-number" href="/page/14/">14</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/13/">Nächste Seite &raquo;</a>
  </nav>

</nav>
</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Google Search">
    <input type="hidden" name="q" value="">
  </form>
</div>

<div class="search">
   <form class="search" action="/search/index.html" method="get" accept-charset="utf-8">
     <input type="search" id="search" class="st-default-search-input" style="height:40px" maxlength="20" placeholder="WebSite Search" />
     <input type="hidden" name="q" value="">
   </form>
</div>



  
  <div class="widget tag">
    <h3 class="title">Archive</h3>
    <div class="entry">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">June 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/02/">February 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">January 2015</a></li></ul>
    </div>
  </div>



  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/tags/Algorithm/">Algorithm</a><small>39</small></li>
  
    <li><a href="/tags/Android/">Android</a><small>1</small></li>
  
    <li><a href="/tags/Bit-Manipulation/">Bit Manipulation</a><small>1</small></li>
  
    <li><a href="/tags/Business-Model/">Business Model</a><small>1</small></li>
  
    <li><a href="/tags/C/">C</a><small>3</small></li>
  
    <li><a href="/tags/C/">C++</a><small>2</small></li>
  
    <li><a href="/tags/Cloud-Computing/">Cloud Computing</a><small>26</small></li>
  
    <li><a href="/tags/Combinatorics/">Combinatorics</a><small>1</small></li>
  
    <li><a href="/tags/Data-Analysis/">Data Analysis</a><small>1</small></li>
  
    <li><a href="/tags/Database/">Database</a><small>9</small></li>
  
    <li><a href="/tags/Graph-Theory/">Graph Theory</a><small>1</small></li>
  
    <li><a href="/tags/IOS/">IOS</a><small>1</small></li>
  
    <li><a href="/tags/Interview-Summary/">Interview Summary</a><small>2</small></li>
  
    <li><a href="/tags/Java/">Java</a><small>12</small></li>
  
    <li><a href="/tags/Kotlin/">Kotlin</a><small>1</small></li>
  
    <li><a href="/tags/Life/">Life</a><small>3</small></li>
  
    <li><a href="/tags/Life-Thoughts/">Life Thoughts</a><small>1</small></li>
  
    <li><a href="/tags/Linux/">Linux</a><small>11</small></li>
  
    <li><a href="/tags/Lua/">Lua</a><small>5</small></li>
  
    <li><a href="/tags/Mac/">Mac</a><small>1</small></li>
  
    <li><a href="/tags/Machine-Learning/">Machine Learning</a><small>11</small></li>
  
    <li><a href="/tags/Natural-Language-Processing/">Natural Language Processing</a><small>1</small></li>
  
    <li><a href="/tags/Python/">Python</a><small>4</small></li>
  
    <li><a href="/tags/R/">R</a><small>1</small></li>
  
    <li><a href="/tags/Redis/">Redis</a><small>1</small></li>
  
    <li><a href="/tags/SparkSQL/">SparkSQL</a><small>1</small></li>
  
    <li><a href="/tags/String/">String</a><small>1</small></li>
  
    <li><a href="/tags/System-Design/">System Design</a><small>7</small></li>
  
    <li><a href="/tags/Test/">Test</a><small>2</small></li>
  
    <li><a href="/tags/Tools/">Tools</a><small>4</small></li>
  
    <li><a href="/tags/Web/">Web</a><small>17</small></li>
  
  </ul>
</div>


  
<div class="widget tagcloud">
  <h3 class="title">Tag-Cloud</h3>
  <div class="entry">
    <a href="/tags/Algorithm/" style="font-size: 20px;">Algorithm</a> <a href="/tags/Android/" style="font-size: 10px;">Android</a> <a href="/tags/Bit-Manipulation/" style="font-size: 10px;">Bit Manipulation</a> <a href="/tags/Business-Model/" style="font-size: 10px;">Business Model</a> <a href="/tags/C/" style="font-size: 11.82px;">C</a> <a href="/tags/C/" style="font-size: 10.91px;">C++</a> <a href="/tags/Cloud-Computing/" style="font-size: 19.09px;">Cloud Computing</a> <a href="/tags/Combinatorics/" style="font-size: 10px;">Combinatorics</a> <a href="/tags/Data-Analysis/" style="font-size: 10px;">Data Analysis</a> <a href="/tags/Database/" style="font-size: 15.45px;">Database</a> <a href="/tags/Graph-Theory/" style="font-size: 10px;">Graph Theory</a> <a href="/tags/IOS/" style="font-size: 10px;">IOS</a> <a href="/tags/Interview-Summary/" style="font-size: 10.91px;">Interview Summary</a> <a href="/tags/Java/" style="font-size: 17.27px;">Java</a> <a href="/tags/Kotlin/" style="font-size: 10px;">Kotlin</a> <a href="/tags/Life/" style="font-size: 11.82px;">Life</a> <a href="/tags/Life-Thoughts/" style="font-size: 10px;">Life Thoughts</a> <a href="/tags/Linux/" style="font-size: 16.36px;">Linux</a> <a href="/tags/Lua/" style="font-size: 13.64px;">Lua</a> <a href="/tags/Mac/" style="font-size: 10px;">Mac</a> <a href="/tags/Machine-Learning/" style="font-size: 16.36px;">Machine Learning</a> <a href="/tags/Natural-Language-Processing/" style="font-size: 10px;">Natural Language Processing</a> <a href="/tags/Python/" style="font-size: 12.73px;">Python</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/Redis/" style="font-size: 10px;">Redis</a> <a href="/tags/SparkSQL/" style="font-size: 10px;">SparkSQL</a> <a href="/tags/String/" style="font-size: 10px;">String</a> <a href="/tags/System-Design/" style="font-size: 14.55px;">System Design</a> <a href="/tags/Test/" style="font-size: 10.91px;">Test</a> <a href="/tags/Tools/" style="font-size: 12.73px;">Tools</a> <a href="/tags/Web/" style="font-size: 18.18px;">Web</a>
  </div>
</div>


  

  <div class="widget tag">
  <h3 class="title">Friend Links</h3>
  <ul class="entry">
    <li><a href="http://blog.haosdent.me/"  target="_blank">Haosong Huang's Blog</a></li>
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">Recent Posts</h3>
  <ul class="entry">
    
      <li>
        <a href="/2017/03/12/Deploy-Cron-Job-on-Mac-Tutorial/">Deploy Cron Job on Mac Tutorial</a>
      </li>
    
      <li>
        <a href="/2017/03/06/Bash-vs-Zsh/">Bash vs Zsh</a>
      </li>
    
      <li>
        <a href="/2017/02/27/Java-Immutable-Mutable-Class-Library/">Java Immutable/Mutable Class Library</a>
      </li>
    
      <li>
        <a href="/2017/02/15/Assert-Expected-Exception-in-JUnit-Test/">Assert Expected Exception in JUnit Test</a>
      </li>
    
      <li>
        <a href="/2017/02/12/SPOJ-Basic-and-Tutorial-Solution-Summary-List/">SPOJ Basic and Tutorial Solution Summary List</a>
      </li>
    
  </ul>
</div>



  <div id="recentcomments" class="widget tag">
    <h3 class="title">Recent Comments</h3>
    <div class="entry">
      <div id="recentcomments" class="dsq-widget">
        <script type="text/javascript" src="http://yular.disqus.com/recent_comments_widget.js?num_items=10&hide_avatars=1&avatar_size=32&excerpt_length=100&hide_mods=0"></script>
      </div>
    </div>
</div>


  <div class="widget tag">
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <h3 class="title">Statics</h3>
  <ul class="entry">   
    <li><span id="busuanzi_container_site_pv">Total Visits: <span id="busuanzi_value_site_pv"></span></span></li>
    <li><span id="busuanzi_container_site_uv">Total Visitors: <span id="busuanzi_value_site_uv"></span></span></li>
  </div>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  <p>
  
  &copy; 2017 BaiChuan Yang
  
  All rights reserved.</p>
  <p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></p>
</div>
<div class="alignright">
   <div style="font-family: FontAwesome;font-size: 20px;">
     <a href="http://weibo.com/u/5854007817?is_all=1" title="weibo" target="_blank"><img src="/css/images/weibo-social-logo.png"></a>&nbsp;
     <a href="https://www.facebook.com/johnson.green.338" title="Facebook" target="_blank"><img src="/css/images/facebook-logo.png"></a>&nbsp;
     <a href="https://twitter.com/JohnsonGreen5" title="twitter" target="_blank"><img src="/css/images/twitter-logo.png"></a>&nbsp;
     <a href="https://www.linkedin.com/in/baichuan-yang-6b7b6579" title="LinkedIn" target="_blank"><img src="/css/images/linkedin-logo.png"></a>&nbsp;
     <a href="https://github.com/yular" title="GitHub" target="_blank"><img src="/css/images/github-sign.png"></a>&nbsp;
     <a href="https://plus.google.com/u/0/+BaichuanYANG/" title="Google+" target="_blank"><img src="/css/images/social-google-plus-square-button.png"></a>
  </div>
</div>
<div class="clearfix"></div>

<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":false,"bdPic":"","bdStyle":"2","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>
</footer>
  <script src="/js/jquery-1.12.1.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


    <script type="text/javascript">
    var disqus_shortname = 'yular';

    (function(){
      var dsq = document.createElement('script');
      dsq.type = 'text/javascript';
      dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
    </script>

<div id="totop" style="position:fixed;bottom:150px;right:50px;cursor: pointer;">
<a title="backtotop"><img src="/imgs/scrollup.png"/></a>
</div>

<script src="/js/totop.js"></script>
<div id='bg'></div>

</body>
</html>